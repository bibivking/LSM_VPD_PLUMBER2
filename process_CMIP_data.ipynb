{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10518ac4-20e8-4b1f-971d-68146b5ca7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy import stats, interpolate\n",
    "import cftime\n",
    "import pandas as pd\n",
    "import netCDF4 as nc\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.cm import get_cmap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib import colors\n",
    "import matplotlib.ticker as mticker\n",
    "# from PLUMBER2_VPD_common_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb33f0b-21c6-4a67-813d-81bc560079ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_quality_control(varname, data_input,zscore_threshold=2):\n",
    "\n",
    "    '''\n",
    "    Please notice EF has nan values\n",
    "    '''\n",
    "\n",
    "    z_scores    = np.abs(stats.zscore(data_input, nan_policy='omit'))\n",
    "    data_output = np.where(z_scores > zscore_threshold, np.nan, data_input)\n",
    "\n",
    "    # print('z_scores',z_scores)\n",
    "    if 'EF' not in varname:\n",
    "        print('EF is not in ', varname)\n",
    "        # Iterate through the data to replace NaN with the average of nearby non-NaN values\n",
    "        for i in range(1, len(data_output) - 1):\n",
    "            if np.isnan(data_output[i]):\n",
    "                prev_index = i - 1\n",
    "                next_index = i + 1\n",
    "\n",
    "                # find the closest non nan values\n",
    "                while prev_index >= 0 and np.isnan(data_output[prev_index]):\n",
    "                    prev_index -= 1\n",
    "\n",
    "                while next_index < len(data_output) and np.isnan(data_output[next_index]):\n",
    "                    next_index += 1\n",
    "\n",
    "                # use average them\n",
    "                if prev_index >= 0 and next_index < len(data_output):\n",
    "                    prev_non_nan = data_output[prev_index]\n",
    "                    next_non_nan = data_output[next_index]\n",
    "                    data_output[i] = (prev_non_nan + next_non_nan) / 2.0\n",
    "\n",
    "    print('len(z_scores)',len(z_scores))\n",
    "    # print('data_output',data_output)\n",
    "\n",
    "    return data_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae51f556-34c1-4816-af0c-4383c92bda8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lat_lon(site_names, PLUMBER2_met_path):\n",
    "\n",
    "    # file path\n",
    "    lat_dict      = {}\n",
    "    lon_dict      = {}\n",
    "\n",
    "    for site_name in site_names:\n",
    "\n",
    "        # Set input file path\n",
    "        file_path = glob.glob(PLUMBER2_met_path+\"/*\"+site_name+\"*.nc\")\n",
    "\n",
    "        f         = nc.Dataset(file_path[0], mode='r')\n",
    "        lat_tmp   = f.variables['latitude'][0,0].data\n",
    "        lon_tmp   = f.variables['longitude'][0,0].data\n",
    "\n",
    "        # convert the array to a floating-point number\n",
    "        lat_dict[site_name] = float(lat_tmp)\n",
    "        lon_dict[site_name] = float(lon_tmp)\n",
    "        f.close()\n",
    "\n",
    "    return lat_dict, lon_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9505b4-9db1-4449-80bc-f7dc22d83fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_mask(time_tmp, time_s, time_e, seconds=None):\n",
    "\n",
    "    '''\n",
    "    Checked on 14 Dec 2021, no problem was identified\n",
    "    '''\n",
    "\n",
    "    # #print(\"In time_mask\")\n",
    "    Time    = time_tmp - datetime(2000,1,1,0,0,0)\n",
    "    Time_s  = time_s - datetime(2000,1,1,0,0,0)\n",
    "    Time_e  = time_e - datetime(2000,1,1,0,0,0)\n",
    "    # print('Time',Time)\n",
    "    # print('Time_s',Time_s)\n",
    "    if seconds == None:\n",
    "        time_cood = (Time>=Time_s) & (Time<Time_e)\n",
    "    else:\n",
    "        time_cood = []\n",
    "        for j in np.arange(len(Time)):\n",
    "            if seconds[0] >= seconds[1]:\n",
    "                if_seconds = (Time[j].seconds >= seconds[0]) | (Time[j].seconds < seconds[1])\n",
    "            else:\n",
    "                if_seconds = (Time[j].seconds >= seconds[0]) & (Time[j].seconds < seconds[1])\n",
    "            time_cood.append( (Time[j]>=Time_s) & (Time[j]<Time_e) & if_seconds)\n",
    "\n",
    "    return time_cood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a115cbb4-1ffb-41a8-ae46-1b19eff2d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files \n",
    "PLUMBER2_met_path = \"/g/data/w97/mm3972/data/Fluxnet_data/Post-processed_PLUMBER2_outputs/Nc_files/Met/\"\n",
    "CMIP6_data_path   = \"/g/data/w97/amu561/CMIP6_for_Mengyuan/Processed_CMIP6_data/\"\n",
    "CMIP6_out_path    = \"/g/data/w97/mm3972/scripts/PLUMBER2/LSM_VPD_PLUMBER2/nc_files/CMIP6/\"\n",
    "scenarios         = ['historical']#,'ssp126','ssp245','ssp585']\n",
    "var_names         = ['hfls','hfss','hurs','tas']\n",
    "\n",
    "# The site names\n",
    "all_site_path     = sorted(glob.glob(PLUMBER2_met_path+\"/*.nc\"))\n",
    "site_names        = [os.path.basename(site_path).split(\"_\")[0] for site_path in all_site_path]\n",
    "\n",
    "\n",
    "# Read variable attributions info from input\n",
    "lat_dict, lon_dict = read_lat_lon(site_names, PLUMBER2_met_path)\n",
    "\n",
    "# get file names\n",
    "file_names           = {}\n",
    "file_names_scenario  = {}\n",
    "\n",
    "for scenario in scenarios:\n",
    "    for var_name in var_names:\n",
    "        file_names_scenario[var_name] = sorted(glob.glob(f'{CMIP6_data_path}{scenario}/{var_name}/*/*/*.nc'))\n",
    "    file_names[scenario] = file_names_scenario\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b54a1e2-c47d-46a0-8237-fad6444f4b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_dict, lon_dict = read_lat_lon(site_names, PLUMBER2_met_path)\n",
    "time_s             = datetime(1950,1,1,0,0,0)\n",
    "time_e             = datetime(2010,1,1,0,0,0)\n",
    "# time_s             = datetime(2060,1,1,0,0,0)\n",
    "# time_e             = datetime(2100,1,1,0,0,0)\n",
    "# for site_name in site_names:\n",
    "#     # get site lat and lon\n",
    "#     lat, lon = read_lat_lon(site_names, PLUMBER2_met_path)\n",
    "\n",
    "#     # read CMIP6 data\n",
    "#     read_CMIP6_data(site_name, file_names, scenarios, var_names, \n",
    "#                     lat=lat_dict[site_name], lon=lon_dict[site_name],\n",
    "#                     time_s=time_s, time_e=time_e)\n",
    "site_name = site_names[0]\n",
    "lat=lat_dict[site_name]\n",
    "lon=lon_dict[site_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90850abb-ab21-4638-ada6-752272ba569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_CMIP6_data(site_name, file_names, scenarios, var_names, lat=None, lon=None):\n",
    "\n",
    "# select the site information from each CMIP6 file\n",
    "\n",
    "for scenario in scenarios:\n",
    "\n",
    "    file_names_scenario = file_names[scenario]\n",
    "    output_file         = CMIP6_out_path+site_name+'_'+scenario+'.nc'\n",
    "    print('Output file is ', output_file)\n",
    "    \n",
    "#     for var_name in var_names:\n",
    "\n",
    "#         file_names_scenario_variable = file_names_scenario[var_name]\n",
    "#         model_out_list               = []\n",
    "        \n",
    "#         for file_name in file_names_scenario_variable:\n",
    "            \n",
    "#             print('file_name',file_name)\n",
    "#             # ! ncdump -h {file_name}\n",
    "#             model_out_name = file_name.split(\"/\")[9]\n",
    "#             model_out_list.append(model_out_name)\n",
    "            \n",
    "#             # Get model name \n",
    "#             f         = nc.Dataset(file_name, mode='r')\n",
    "\n",
    "#             # Read lat and lon\n",
    "#             try:\n",
    "#                 latitude  = f.variables['lat'][:]\n",
    "#                 longitude = f.variables['lon'][:]\n",
    "#             except:\n",
    "#                 latitude  = f.variables['latitude'][:]\n",
    "#                 longitude = f.variables['longitude'][:]\n",
    "\n",
    "#             # Read time \n",
    "#             time_tmp  = nc.num2date(f.variables['time'][:],f.variables['time'].units,\n",
    "#                         only_use_cftime_datetimes=False, calendar=f.variables['time'].calendar) # only_use_python_datetimes=True,\n",
    "\n",
    "#             # To solve the inconsistancy in time coordinate \n",
    "#             for i, t in enumerate(time_tmp):\n",
    "#                 year   = t.year\n",
    "#                 month  = t.month\n",
    "#                 day    = t.day\n",
    "#                 hour   = t.hour\n",
    "#                 minute = t.minute\n",
    "#                 second = t.second\n",
    "#                 microsecond = t.microsecond\n",
    "#                 time_tmp[i] = datetime(year, month, day, hour, minute, second, microsecond)\n",
    "            \n",
    "#             # select time periods\n",
    "#             time_cood = time_mask(time_tmp, time_s, time_e)        \n",
    "            \n",
    "#             # make new time cooridate \n",
    "#             time_tmp  = time_tmp[time_cood]\n",
    "            \n",
    "#             # Read variable\n",
    "#             var_tmp = f.variables[var_name][:]\n",
    "#             var_units = f.variables[var_name].units\n",
    "#             var_long_name = f.variables[var_name].long_name\n",
    "#             lat_idx = np.argmin(np.abs(latitude - lat))\n",
    "#             lon_idx = np.argmin(np.abs(longitude - lon))\n",
    "#             var     = var_tmp[time_cood, lat_idx, lon_idx]\n",
    "\n",
    "#             # Make nc file \n",
    "#             if not os.path.exists(output_file):\n",
    "                \n",
    "#                 # make output file\n",
    "#                 f = nc.Dataset(output_file, 'w', format='NETCDF4')\n",
    "                \n",
    "#                 ### Create nc file ###\n",
    "#                 # f.history           = \"Created by: %s\" % (os.path.basename(__file__))\n",
    "#                 f.creation_date     = \"%s\" % (datetime.now())\n",
    "#                 f.description       = 'CMIP6 '+scenario+' at '+site_name+', made by MU Mengyuan'\n",
    "#                 f.Conventions       = \"CF-1.0\"\n",
    "\n",
    "#                 # set time dimensions\n",
    "#                 ntime               = len(var)\n",
    "#                 Time_name           = 'time'\n",
    "#                 f.createDimension(Time_name, ntime)\n",
    "\n",
    "                \n",
    "#                 time_output = []\n",
    "#                 for t_tmp in time_tmp:\n",
    "#                     time_output.append((t_tmp - datetime(2000,1,1,0,0,0)).days)\n",
    "\n",
    "#                 Time                = f.createVariable(Time_name, 'f4', (Time_name))\n",
    "#                 Time.standard_name  = Time_name\n",
    "#                 Time.units          = 'days since 2000-01-01 00:00:00'\n",
    "#                 Time[:]             = time_output\n",
    "\n",
    "#                 Var_name            = model_out_name+\"_\"+var_name\n",
    "#                 Var                 = f.createVariable(Var_name, 'f4', (Time_name))\n",
    "#                 Var.standard_name   = Var_name\n",
    "#                 Var.units           = var_units\n",
    "#                 Var.long_name       = var_long_name\n",
    "#                 Var[:]              = var[:]\n",
    "\n",
    "#                 f.close()\n",
    "        \n",
    "#             else:\n",
    "#                 # add to the exist nct file\n",
    "#                 f = nc.Dataset(output_file, 'r+', format='NETCDF4')\n",
    "\n",
    "#                 # set dimensions\n",
    "#                 ntime               = len(var)\n",
    "#                 Time_name           = 'time'\n",
    "#                 Var_name            = model_out_name+\"_\"+var_name\n",
    "\n",
    "#                 # if it doesn't exist then create\n",
    "#                 if Var_name not in f.variables:\n",
    "#                     Var                = f.createVariable(Var_name, 'f4', (Time_name))\n",
    "#                     Var.standard_name  = Var_name\n",
    "#                     Var.units          = var_units\n",
    "#                     Var.long_name      = var_long_name\n",
    "#                     Var[:]             = var[:]\n",
    "#                 else:\n",
    "#                     f.variables[Var_name][:]    = var[:]\n",
    "#                     f.variables[Var_name].units = var_units\n",
    "\n",
    "#                 f.close()\n",
    "                \n",
    "                \n",
    "#         # Add model list\n",
    "#         f = nc.Dataset(output_file, 'r+', format='NETCDF4')\n",
    "#         model_list_name = var_name+\"_models\"\n",
    "#         model_out_num   = len(model_out_list)\n",
    "#         model_names_array= np.array(model_out_list, dtype=\"S20\")\n",
    "#         if model_list_name not in f.variables:\n",
    "#             # set model names dimension\n",
    "#             f.createDimension(model_list_name, model_out_num)\n",
    "\n",
    "#             # create variables\n",
    "#             model               = f.createVariable(model_list_name, \"S20\", (model_list_name))\n",
    "#             model.standard_name = model_list_name\n",
    "#             model[:]            = model_names_array\n",
    "#         f.close()\n",
    "#         gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14c90ad-4087-4d1a-b26c-4414c01c7760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_EF_to_nc_file(output_file, zscore_threshold=2, Qle_Qh_threshold=10):\n",
    "    # output_file = '/g/data/w97/mm3972/scripts/PLUMBER2/LSM_VPD_PLUMBER2/nc_files/CMIP6/AR-SLu_historical.nc'\n",
    "    # zscore_threshold=2\n",
    "    # Qle_Qh_threshold=10\n",
    "    # Set input file path\n",
    "    f_out                = nc.Dataset(output_file,'r+')\n",
    "    latent_models        = f_out.variables['hfls_models'][:]\n",
    "    sensible_models      = f_out.variables['hfss_models'][:]\n",
    "\n",
    "    model_out_names      = []\n",
    "    model_out_num        = 0\n",
    "\n",
    "    # check whether both latent and sensible fluxes exist in the model\n",
    "    for latent_model in latent_models:\n",
    "        if latent_model in sensible_models:\n",
    "\n",
    "            model_out_names.append(latent_model)\n",
    "            model_out_num  = model_out_num + 1\n",
    "\n",
    "            # print(latent_model, 'has both Qle and Qh')\n",
    "\n",
    "            f_out          = nc.Dataset(output_file,'r+')\n",
    "            model_sensible = f_out.variables[latent_model+'_hfls'][:]\n",
    "            model_latent   = f_out.variables[latent_model+'_hfss'][:]\n",
    "            print('model_sensible',model_sensible)\n",
    "            model_EF_tmp   = np.where(np.all([model_sensible+model_latent > Qle_Qh_threshold , model_sensible>0],axis=0),\n",
    "                                      model_latent/(model_sensible+model_latent), np.nan)\n",
    "            model_EF_tmp   = np.where(model_EF_tmp<0,np.nan,model_EF_tmp)\n",
    "            model_EF_tmp   = conduct_quality_control('EF',model_EF_tmp,zscore_threshold)\n",
    "\n",
    "            # Try to access the variable\n",
    "            try:\n",
    "                # if model_EF exists, update to the new values\n",
    "                f_out.variables[latent_model+'_EF'][:] = model_EF_tmp\n",
    "            except:\n",
    "                # if model_EF doesn't exist, create the model_EF\n",
    "                model_EF                = f_out.createVariable(latent_model+'_EF', 'f4', ('time'))\n",
    "                model_EF.standard_name  = latent_model+\"_EF\"\n",
    "                model_EF.long_name      = \"Evaporative fraction (hfls/(hfls+hfss)) in \"+latent_model\n",
    "                model_EF.units          = \"-\"\n",
    "                model_EF[:]             = model_EF_tmp\n",
    "            f_out.close()\n",
    "\n",
    "    # output the model has both Qle and Qh\n",
    "    f_out               = nc.Dataset(output_file,'r+')\n",
    "\n",
    "    try:\n",
    "        # if model_EF exists, update to the new values\n",
    "        print('EF_models:', f_out.variables['EF_models'])\n",
    "    except:\n",
    "        # set model names dimension\n",
    "        f_out.createDimension(\"EF_models\", model_out_num)\n",
    "\n",
    "        # Form the model names array\n",
    "        model_names_array   = np.array(model_out_names, dtype=\"S20\")\n",
    "\n",
    "        # create variables\n",
    "        model               = f_out.createVariable(\"EF_models\", \"S20\", (\"EF_models\"))\n",
    "        model.standard_name = \"EF_models\"\n",
    "        model[:]            = model_names_array\n",
    "\n",
    "    f_out.close()\n",
    "\n",
    "    # return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8c2634-6462-46a6-88ae-191b99f68899",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_EF_to_nc_file(output_file, zscore_threshold=2, Qle_Qh_threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ac19c4-6b26-4a01-8443-0981aef898fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_vpd_to_nc_file(output_file):\n",
    "output_file = '/g/data/w97/mm3972/scripts/PLUMBER2/LSM_VPD_PLUMBER2/nc_files/CMIP6/AR-SLu_historical.nc'\n",
    "\n",
    "# Set input file path\n",
    "f_out                = nc.Dataset(output_file,'r+')\n",
    "RH_models            = f_out.variables['hurs_models'][:]\n",
    "Tair_models          = f_out.variables['tas_models'][:]\n",
    "\n",
    "model_out_names      = []\n",
    "model_out_num        = 0\n",
    "\n",
    "# check whether both latent and sensible fluxes exist in the model\n",
    "for RH_model in RH_models:\n",
    "    if RH_model in Tair_models:\n",
    "\n",
    "        model_out_names.append(RH_model)\n",
    "        model_out_num  = model_out_num + 1\n",
    "\n",
    "        # print(latent_model, 'has both Qle and Qh')\n",
    "\n",
    "        f_out          = nc.Dataset(output_file,'r+')\n",
    "        model_RH       = f_out.variables[RH_model+'_hurs'][:]\n",
    "        model_Tair     = f_out.variables[RH_model+'_tas'][:]\n",
    "\n",
    "        # model_VPD_tmp  = \n",
    "\n",
    "#         # Try to access the variable\n",
    "#         try:\n",
    "#             # if model_EF exists, update to the new values\n",
    "#             f_out.variables[latent_model+'_VPD'][:] = model_VPD_tmp\n",
    "#         except:\n",
    "#             # if model_EF doesn't exist, create the model_EF\n",
    "#             model_VPD                = f_out.createVariable(RH_model+'_VPD', 'f4', ('time'))\n",
    "#             model_VPD.standard_name  = latent_model+\"_VPD\"\n",
    "#             model_VPD.long_name      = \"Vapor Pressure Deficit in \"+RH_model\n",
    "#             model_VPD.units          = \"-\"\n",
    "#             model_VPD[:]             = model_VPD_tmp\n",
    "#         f_out.close()\n",
    "\n",
    "# # output the model has both Qle and Qh\n",
    "# f_out               = nc.Dataset(output_file,'r+')\n",
    "\n",
    "# try:\n",
    "#     # if model_EF exists, update to the new values\n",
    "#     print('VPD_models:', f_out.variables['VPD_models'])\n",
    "# except:\n",
    "#     # set model names dimension\n",
    "#     f_out.createDimension(\"VPD_models\", model_out_num)\n",
    "\n",
    "#     # Form the model names array\n",
    "#     model_names_array   = np.array(model_out_names, dtype=\"S20\")\n",
    "\n",
    "#     # create variables\n",
    "#     model               = f_out.createVariable(\"VPD_models\", \"S20\", (\"VPD_models\"))\n",
    "#     model.standard_name = \"VPD_models\"\n",
    "#     model[:]            = model_names_array\n",
    "\n",
    "# f_out.close()\n",
    "\n",
    "    # return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb612652-7575-44f8-9be7-3d1896336207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
